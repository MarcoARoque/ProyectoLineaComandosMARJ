# -*- coding: utf-8 -*-
"""ProyectoFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Etu3b0wvQXLmeq0ogXBJz4_a23_KPocD
"""

import pandas as pd
file = "facebook_data.json"
DataFrame1 = pd.read_json(file,orient='records')
print(DataFrame1)

pip install numpy

pip install pandas

import numpy as np
import pandas as pd

facebook_data=pd.read_json("C:/Users/DELL/LineaDeComandos/ProyectoLineaComandosMARJ/ProyectoLineaComandosFinal/CodigoFuente/Datos/facebook_data.json")
instagram_data=pd.read_json("C:/Users/DELL/LineaDeComandos/ProyectoLineaComandosMARJ/ProyectoLineaComandosFinal/CodigoFuente/Datos/instagram_data.json")
twitter_data=pd.read_json("C:/Users/DELL/LineaDeComandos/ProyectoLineaComandosMARJ/ProyectoLineaComandosFinal/CodigoFuente/Datos/twitter_data.json")

print(facebook_data)

print (instagram_data)

print(twitter_data)

!pip install nltk

import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd

def preprocesar_texto(texto):
  tokens = word_tokenize(texto.lower())

  stop_words = set(stopwords.words('spanish'))
  tokens = [word for word in tokens if word.isalnum() and word not in stop_words]

  lemmatizer = WordNetLemmatizer()
  tokens = [lemmatizer.lemmatize(word) for word in tokens]

  return ' '.join(tokens)

facebook_data['contenido'] = facebook_data['contenido'].apply(preprocesar_texto)

print(facebook_data)

pip install scikit-learn

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(facebook_data['contenido'])
y = facebook_data['likes']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo = LogisticRegression()
modelo.fit(X_train, y_train)

y_pred = modelo.predict(X_test)
print("Precisión:", accuracy_score(y_test, y_pred))
print("Reporte de Clasificación:\n", classification_report(y_test, y_pred))

!pip install spacy

!python -m spacy download es_core_news_sm

import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

nlp = spacy.load('es_core_news_sm')

def preprocesar_texto(texto):
    doc = nlp(texto)
    return " ".join([token.lemma_ for token in doc if not token.is_stop])

textos = facebook_data['contenido'].tolist()  # Extract the 'text' column as a list
textos_procesados = [preprocesar_texto(texto) for texto in textos]
rating = facebook_data['likes']

# Vectorizar los textos
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos_procesados)
y = rating

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar un modelo SVM
modelo = SVC(kernel='linear')
modelo.fit(X_train, y_train)

# Evaluar el modelo
y_pred = modelo.predict(X_test)
print(y_pred)
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Precisión:", accuracy_score(y_test, y_pred))import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

nlp = spacy.load('es_core_news_sm')

def preprocesar_texto(texto):
    doc = nlp(texto)
    return " ".join([token.lemma_ for token in doc if not token.is_stop])

textos = facebook_data['contenido'].tolist()  # Extract the 'text' column as a list
textos_procesados = [preprocesar_texto(texto) for texto in textos]
rating = facebook_data['likes']

# Vectorizar los textos
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos_procesados)
y = rating

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar un modelo SVM
modelo = SVC(kernel='linear')
modelo.fit(X_train, y_train)

# Evaluar el modelo
y_pred = modelo.predict(X_test)
print(y_pred)
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
# print("Precisión:", accuracy_score(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(facebook_data['contenido'])
y = facebook_data['contenido']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo = LogisticRegression()
modelo.fit(X_train, y_train)

y_pred = modelo.predict(X_test)
print("Precisión:", accuracy_score(y_test, y_pred))
print("Reporte de Clasificación:\n", classification_report(y_test, y_pred))

!python -m spacy download es_core_news_sm

import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

nlp = spacy.load('es_core_news_sm')

def preprocesar_texto(texto):
    doc = nlp(texto)
    return " ".join([token.lemma_ for token in doc if not token.is_stop])

textos = facebook_data['contenido'].tolist()  # Extract the 'text' column as a list
textos_procesados = [preprocesar_texto(texto) for texto in textos]
rating = facebook_data['likes']

# Vectorizar los textos
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos_procesados)
y = rating

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar un modelo SVM
modelo = SVC(kernel='linear')
modelo.fit(X_train, y_train)

# Evaluar el modelo
y_pred = modelo.predict(X_test)
print(y_pred)
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Precisión:", accuracy_score(y_test, y_pred))

pip install transformers

pip install TensorFlow

pip install tf_keras

from transformers import pipeline

sentiment_analysis = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

title = facebook_data['contenido'].tolist()
resultados = sentiment_analysis(title)

for texto, resultado in zip(title, resultados):
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['label']} (Score: {resultado['score']:.2f})\n")

from transformers import pipeline

sentiment_analysis = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

test = facebook_data['contenido'].tolist()
resultados = sentiment_analysis(test)

for texto, resultado in zip(title, resultados):
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['label']} (Score: {resultado['score']:.2f})\n")

